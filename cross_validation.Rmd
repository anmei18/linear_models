---
title: "Cross validation"
author: "AnMei Chen"
date: "11/16/2021"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis" , 
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## simulate data


```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - .3) ^ 2 + rnorm(100, 0, .3)
  )
```

look at the data

```{r}
nonlin_df %>% 
  ggplot(aes(x = x , y = y)) +
  geom_point()
```

## cross validation -- by hand

get training and testing datasets

training df sampled 80 of samples from the nonlin_df.
testing df gets 20 samples from the nonlin_df that is not included in the train_df

```{r}

train_df = sample_n(nonlin_df, size = 80)
test_df = anti_join(nonlin_df, train_df, by = "id")

```

Fit three models.

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = gam( y ~ s(x), data = train_df)
wiggly_mod = gam( y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

can I see what I just did...

```{r}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes( y = pred), color = "red")

train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes( y = pred), color = "red")


train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes( y = pred), color = "red")
```

smooth model is the best in terms of accuracy.

use gather_predictions() and facet_grid to compare models simultaneously

```{r}
train_df %>% 
  gather_predictions(linear_mod,smooth_mod, wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes( y = pred), color = "red") +
  facet_grid(. ~ model)
```

Look at prediction accuracy.

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```


## cross validation using `modelr`

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100)
```

what is happening here ...

```{r}
cv_df %>% 
  pull(train) %>% 
  .[[1]] %>% 
  as_tibble()

cv_df %>% 
  pull(test) %>% 
  .[[1]] %>% 
  as_tibble()

cv_df = 
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test,as_tibble)
  )
```


Let's try to fit models and get RMSEs for them.

Map across "train" column and fit y against x with linear models into each elements in train column.
use .x as a placeholder of train.

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    linear_mod = map(.x = train, ~lm( y ~ x, data = .x)),
    smooth_mod = map(.x = train, ~gam( y ~ s(x), data = .x)),
    wiggly_mod = map(.x = train, ~gam( y ~ s(x, k = 30), sp = 10e-6, data = .x))
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_mod, .y = test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(.x = smooth_mod, .y = test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(.x = wiggly_mod, .y = test, ~rmse(model = .x, data = .y))
  )
```

what dod these results say about model choices?

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

This is the distributions across 100 training and testing splits fitting in the linear, smooth, wiggly models.
From this plot we can see that the linear model makes the worst prediction. The smooth is doing better than the wiggly.

Compute averages ...

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  group_by(model) %>% 
  summarize((avg_smse = mean(rmse)))
```


